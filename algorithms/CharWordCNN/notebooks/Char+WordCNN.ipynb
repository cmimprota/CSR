{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Char+WordCNN.ipynb","provenance":[{"file_id":"155S_bb3viIoL0wAwkIyr1r8XQu4ARwA9","timestamp":1595352852789}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f2b601ed7af54d06abdf11101bc44093":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_454a143d38854c0386470c27113fba98","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cba10a188d0e4cbcbbbfa3ae2dff3c00","IPY_MODEL_27c4ee22a1074e8d8462740c51424443"]}},"454a143d38854c0386470c27113fba98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cba10a188d0e4cbcbbbfa3ae2dff3c00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_329ce3c3b64d4e51b6ebc30acc571768","_dom_classes":[],"description":"  1%","_model_name":"FloatProgressModel","bar_style":"danger","max":17579,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":97,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ab9fdf0e3ce0409ba07d4ba3292343c6"}},"27c4ee22a1074e8d8462740c51424443":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fd69aecc2ef446a889c9c53024523dac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97/17579 [00:14&lt;40:46,  7.15it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b6ed826e11314c7d9fe4e521abacf6c9"}},"329ce3c3b64d4e51b6ebc30acc571768":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ab9fdf0e3ce0409ba07d4ba3292343c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd69aecc2ef446a889c9c53024523dac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b6ed826e11314c7d9fe4e521abacf6c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17143582913e4107a590a6533c72f63f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bf5bfcb7de1c44248152d3a7276d44dc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_00d9d6f06b524088ad04f14a600ef2a1","IPY_MODEL_1d21f2399d864e72804e60bbc9a75897"]}},"bf5bfcb7de1c44248152d3a7276d44dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00d9d6f06b524088ad04f14a600ef2a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1d86802c78b44715989d810c91088af1","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":196,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":196,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e60e37ac9e0459ba4600e073ffc07f2"}},"1d21f2399d864e72804e60bbc9a75897":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b90d607307514e3bbcb7ddf804a777d7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 196/196 [00:30&lt;00:00,  6.52it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49b262cc5c6c499a977ad82124ebdb19"}},"1d86802c78b44715989d810c91088af1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5e60e37ac9e0459ba4600e073ffc07f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b90d607307514e3bbcb7ddf804a777d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"49b262cc5c6c499a977ad82124ebdb19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4db8b704d94e4b6082c063dc2125601d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_75e221184d6e4c939a94974dd6a97c56","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_961a2e1aacb5475e96fd2287c1fbafda","IPY_MODEL_957e2fef4e0e47b4afdb6b25e88c8fab"]}},"75e221184d6e4c939a94974dd6a97c56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"961a2e1aacb5475e96fd2287c1fbafda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d45ef604ec3f446bb12b9758278d8fb3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":196,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":196,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2e787eb186c04e638044e9b20d399031"}},"957e2fef4e0e47b4afdb6b25e88c8fab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85fdcf09b39f45f5b8b84b1c995c592e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 196/196 [00:25&lt;00:00,  7.69it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90e8857c563c451486918be92bdc98da"}},"d45ef604ec3f446bb12b9758278d8fb3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2e787eb186c04e638044e9b20d399031":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85fdcf09b39f45f5b8b84b1c995c592e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"90e8857c563c451486918be92bdc98da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59708c25cd4a4c02ae93e7d5230c17f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_afd9456eff64472983d6baca34ab6b9f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_28482df924a2402caffe14fbb3c01ba6","IPY_MODEL_69425015db064e2dac605622cd552702"]}},"afd9456eff64472983d6baca34ab6b9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28482df924a2402caffe14fbb3c01ba6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c00446e369e2456e99526ac519c9a2e1","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":79,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":79,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa89ea5bb80b4e1188e89e1ca47a92a7"}},"69425015db064e2dac605622cd552702":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0e96860aa7ec48eaaf8ee4b694e0d594","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 79/79 [03:00&lt;00:00,  2.28s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_748daa432f954f34acd2af81f64f72ad"}},"c00446e369e2456e99526ac519c9a2e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fa89ea5bb80b4e1188e89e1ca47a92a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e96860aa7ec48eaaf8ee4b694e0d594":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"748daa432f954f34acd2af81f64f72ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"-ZMIePM2GdD9","colab_type":"text"},"source":["### Imports, preparation"]},{"cell_type":"markdown","metadata":{"id":"wtj-GYGYZaE-","colab_type":"text"},"source":["(Used the trick in https://github.com/googlecolab/colabtools/issues/253#issuecomment-648634717 to obtain more RAM in google colab)"]},{"cell_type":"code","metadata":{"id":"UQlufma4sO4J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1595882086060,"user_tz":-120,"elapsed":6810,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"88200a4c-694c-46b3-e6be-fd110d578a39"},"source":["!pip install -q torchtext==0.6.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 7.5MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WdJEaEg4ZVjG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1595882104711,"user_tz":-120,"elapsed":25346,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"dc049e2b-9fbb-4c56-bd4c-2b68060a71f5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","ROOT_PATH = \"/content/drive/My Drive/cil\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7FUgePxxkFoq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882108254,"user_tz":-120,"elapsed":28713,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["import torch\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch import optim\n","from torch import nn\n","from sklearn import metrics\n","\n","import pandas as pd\n","import numpy as np\n","import joblib\n","\n","import pickle\n","import json\n","import csv\n","\n","import sys\n","import os\n","from tqdm.notebook import tqdm\n","tqdm.pandas()\n","import argparse\n","import datetime\n","import errno"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYiTK-zhGi3x","colab_type":"text"},"source":["### Set experiment parameters"]},{"cell_type":"code","metadata":{"id":"Xouk0DLb2AIZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882794263,"user_tz":-120,"elapsed":1173,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["class Obj:\n","    pass\n","args = Obj()\n","args.checkpoint_save_to_dir = os.path.join(ROOT_PATH, \"CIL-results\", \"my_checkpoints\")\n","args.checkpoint_continue_from = None # os.path.join(ROOT_PATH, \"CIL-results\", \"my_checkpoints\", \"...\") --> just set this inline wherever most convenient\n","args.cuda = True\n","args.epochs = 10\n","args.max_norm = 1e3\n","args.val_interval = 5000 # evaluate on the validation set every val_interval batch\n","args.max_samples = None # TODO: set to None to use all samples\n","args.batch_size = 128\n","args.num_workers = 4\n","args.val_frac = 0.01 # 0.1 # reserve 0.1 = 10% of the training samples for validation.\n","# args.val_frac = None # Set to None to train on full training set, without validation.\n","\n","ALPHABET_SIZE = 70"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-C1vz4UeGhqx","colab_type":"text"},"source":["### Load datasets"]},{"cell_type":"code","metadata":{"id":"qqls5mcGkndx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882799502,"user_tz":-120,"elapsed":721,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["from torchtext.vocab import GloVe\n","\n","class TweetsAsCharsAndWordsDataset(Dataset):\n","    def __init__(self, data_path, alphabet_path, is_labeled=True, l0=501, l1=131, max_samples=None, \n","                 word_emb_name=\"twitter.27B\", word_emb_dim=200, vector_cache_path=os.path.join(ROOT_PATH, \"CIL-aux-data\")):\n","        \"\"\"A dataset object whose samples consist of *both*\n","            - the (padded) concatenation of the word vectors of a tweet, and\n","            - the per-character one-hot encoding of the same tweet.\n","\n","        Arguments:\n","            data_path: path of (label and) data file in csv.\n","            alphabet_path: path of alphabet json file.\n","            is_labeled: whether the data_path file contains labels, or only the tweets.\n","            l1: max length of a sample, in nb of characters.\n","            l1: max length of a sample, in nb of words.\n","            max_samples: (for dev,) only keep the max_samples first samples of the data.\n","            \n","            word_emb_name: name of the word embedding to use, used by torchtext.GloVe.\n","            word_emb_dim: dimension of the word embedding to use, used by torchtext.GloVe.\n","            vector_cache_path: path to cache directory, used by torchtext.GloVe.\n","        \"\"\"\n","        self.glove = GloVe(name=word_emb_name, dim=word_emb_dim, cache=vector_cache_path)\n","        print(\"loaded pretrained GloVe word-embeddings.\")\n","        self.data_path = data_path\n","        self.alphabet_path = alphabet_path\n","        self.is_labeled = is_labeled\n","        self.l0 = l0\n","        self.l1 = l1\n","        with open(alphabet_path) as f:\n","            self.alphabet = ''.join(json.load(f))\n","        self.raw_nb_feats = len(self.alphabet)\n","        self.pro_nb_feats = word_emb_dim\n","        # TODO: setting max_samples only makes sense if the csv itself was shuffled\n","        # X_txt = pd.read_csv(data_path, nrows=max_samples) # only keep max_samples first samples, or keep all if None\n","        X_txt = pd.read_csv(data_path)\n","        if max_samples:\n","            assert is_labeled, \"must not use `max_samples` for unlabeled (assumed test-) data, as shuffling would modify the samples' ordering\"\n","            X_txt = X_txt.sample(frac=1).reset_index(drop=True).iloc[:max_samples] # shuffle then select max_samples first\n","        self.y = X_txt['label'].to_numpy().astype(np.integer, copy=False) if is_labeled else None\n","        self.X_pro = X_txt['preprocessed_segmented_tweet'].to_numpy()\n","        self.X_raw = X_txt['raw_tweet'].to_numpy()\n","            \n","    def __len__(self):\n","        return self.X_raw.shape[0]\n","\n","    def __getitem__(self, idx):\n","        X_raw = self.get_item_raw(idx)\n","        X_pro = self.get_item_pro(idx)\n","        # even if X consists of two distinct parts, still output X,y so that auxiliary functions work without modification\n","        if self.is_labeled:\n","            return (X_raw, X_pro), self.y[idx]\n","        else:\n","            return (X_raw, X_pro)\n","\n","    def get_item_pro(self, idx):\n","        words = self.X_pro[idx].lower().split()\n","        words += [\"\"]*(self.l1 - len(words)) # pad with zeros until of correct size\n","        assert len(words) == self.l1\n","        X = self.glove.get_vecs_by_tokens(words, lower_case_backup=True)\n","        # for i in np.where(~X.bool().all(axis=1))[0]: # print OOV words\n","        #     if words[i] != \"\":\n","        #         print(\"out-of-vocabulary:\", i, words[i])\n","        assert X.shape == (self.l1, self.glove.dim)\n","        return X\n","\n","    def get_item_raw(self, idx):\n","        seq = self.X_raw[idx]\n","        X = self.oneHotEncode(seq)\n","        assert X.shape == (self.l0, self.raw_nb_feats) # NOTE: this is the transpose of what Xiaochen did\n","        return X\n","\n","    def char2idx(self, character):\n","        return self.alphabet.find(character)\n","\n","    def oneHotEncode(self, seq):\n","        X = torch.zeros(self.l0, self.raw_nb_feats)\n","        for i, char in enumerate(seq[::-1]):\n","            char_idx = self.char2idx(char)\n","            if char_idx != -1: # if char is in present in self.alphabet\n","                X[i, char_idx] = 1.0\n","        return X"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"H9-r2lEj1ntW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1595882809408,"user_tz":-120,"elapsed":10440,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"81323cb1-a527-41b1-89dd-1f2f422a49f2"},"source":["ALPHABET_PATH = os.path.join(ROOT_PATH, \"CIL-aux-data\", \"alphabet.json\")\n","PREPROCESSED_TWITTER_DATASETS_DIR = os.path.join(ROOT_PATH, \"stanford_glove_preprocessed\")\n","TWEETS_TRAIN_FILENAME = os.path.join(PREPROCESSED_TWITTER_DATASETS_DIR, \"dataset_stanfordglove_segmented_full.csv\")\n","\n","# train_dataset = TweetsAsCharsAndWordsDataset(TWEETS_TRAIN_FILENAME, ALPHABET_PATH, is_labeled=True)\n","train_dataset = TweetsAsCharsAndWordsDataset(TWEETS_TRAIN_FILENAME, ALPHABET_PATH, is_labeled=True, max_samples=args.max_samples)\n","assert train_dataset.raw_nb_feats == ALPHABET_SIZE\n","\n","if args.val_frac:\n","    val_size = int(args.val_frac * len(train_dataset))\n","    train_size = len(train_dataset) - val_size\n","\n","    torch.manual_seed(0) # need random_split to be deterministic if we want to avoid information leak when we reload notebook in-between training epochs\n","    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","    torch.manual_seed(torch.initial_seed())\n","\n","    val_dataloader = DataLoader(val_dataset, \n","                                batch_size=args.batch_size,\n","                                shuffle=True, \n","                                num_workers=args.num_workers)\n","    \n","train_dataloader = DataLoader(train_dataset, \n","                              batch_size=args.batch_size,\n","                              shuffle=True, \n","                              num_workers=args.num_workers)\n","\n","len(train_dataloader), len(val_dataloader) if args.val_frac else None # number of batches. multiply by args.batch_size to get (approximate) number of samples."],"execution_count":34,"outputs":[{"output_type":"stream","text":["loaded pretrained GloVe word-embeddings.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(19336, 196)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"tzF3pWFIQKqj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1595882124243,"user_tz":-120,"elapsed":43801,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"f63210bf-de55-4af7-d28e-15b604d909b4"},"source":["TWEETS_TEST_FILENAME = os.path.join(PREPROCESSED_TWITTER_DATASETS_DIR, \"test_stanfordglove_segmented.csv\")\n","\n","test_dataset = TweetsAsCharsAndWordsDataset(TWEETS_TEST_FILENAME, ALPHABET_PATH, is_labeled=False)\n","assert test_dataset.raw_nb_feats == ALPHABET_SIZE\n","test_dataloader = DataLoader(test_dataset, \n","                             batch_size=args.batch_size,\n","                             shuffle=False, # need to keep the ordering of the tweets\n","                             num_workers=0)\n","len(test_dataloader)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["loaded pretrained GloVe word-embeddings.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["79"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"qYoyTvrU6uSq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882124246,"user_tz":-120,"elapsed":43684,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["# for i_batch, data in enumerate(train_dataloader):\n","#     (X_raw, X_pro), y = data\n","#     print(i_batch, X_raw.shape, X_pro.shape, y)\n","#     if i_batch == 2:\n","#         break\n","# for i_batch, data in enumerate(val_dataloader):\n","#     (X_raw, X_pro), y = data\n","#     print(i_batch, X_raw.shape, X_pro.shape, y)\n","#     if i_batch == 2:\n","#         break"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0K2FJ4dGHg0","colab_type":"text"},"source":["### Auxiliary functions"]},{"cell_type":"code","metadata":{"id":"N50HMkp6DKa_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882124249,"user_tz":-120,"elapsed":43311,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["def save_checkpoint(model, optimizer, checkpoint, filename):\n","    \"\"\"\n","    Args:\n","        optimizer: can be set to None, then no optimizer will be saved\n","        checkpoint is a dict that can be prepopulated (e.g with keys 'epoch' and 'validation_accuracy')\n","    \"\"\"\n","    # https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices\n","    if isinstance(model, torch.nn.DataParallel):\n","        assert not isinstance(model.module, torch.nn.DataParallel) # check we didn't wrap multiple times by mistake...\n","        checkpoint['model_state_dict'] = model.module.state_dict()\n","    else:\n","        checkpoint['model_state_dict'] = model.state_dict()\n","    if optimizer is not None:\n","        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n","    torch.save(checkpoint,filename)\n","\n","def load_checkpoint(model, optimizer, filename):\n","    \"\"\"\n","    Args:\n","        optimizer: can be set to None, then the optimizer state will be ignored (if there is one stored in the checkpoint)\n","            MUST be set to None if no optimizer state is stored in the checkpoint (so as to minimize risks of confusion)\n","    \"\"\"\n","    # https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices\n","    assert os.path.isfile(filename), f\"no checkpoint found at {filename} (no such file)\"\n","    # try to make it so that cpu->cpu, gpu->gpu, cpu->gpu, gpu->cpu all work (not 100% sure but I think this should do it)\n","    if args.cuda:\n","        device = torch.device(\"cuda\")\n","        checkpoint = torch.load(filename, map_location=device)\n","    else:\n","        device = torch.device(\"cpu\")\n","        checkpoint = torch.load(filename, map_location=device)\n","    # checkpoint = torch.load(filename) # or just don't worry abt it and pray that it works\n","    \n","    if isinstance(model, torch.nn.DataParallel):\n","        assert not isinstance(model.module, torch.nn.DataParallel) # check we didn't wrap multiple times by mistake...\n","        model.module.load_state_dict(checkpoint['model_state_dict'])\n","    else:\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","    if args.cuda:\n","        model = model.cuda() # possibly always a noop but just in case\n","\n","    loaded_optimizer = False\n","    if 'optimizer_state_dict' in checkpoint.keys():\n","        if optimizer is not None:\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","            loaded_optimizer = True\n","    else:\n","        assert optimizer is None, \"Argument `optimizer` MUST be set to None if no optimizer state is stored in the checkpoint\"\n","\n","    if loaded_optimizer:\n","        print(\"successfully loaded model and optimizer states from checkpoint (in place)\")\n","    else:\n","        print(\"successfully loaded model state from checkpoint (in place). (Did NOT load optimizer state.)\")\n","    \n","    if 'epoch' in checkpoint:\n","        print(f\"the model was trained for {checkpoint['epoch']} epochs\")\n","    if 'validation_accuracy' in checkpoint:\n","        print(f\"the model had achieved validation accuracy {checkpoint['validation_accuracy']}\")\n","    return checkpoint\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9iQttFHx1y6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882124251,"user_tz":-120,"elapsed":43142,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["def eval(val_dataloader, model):\n","    criterion_reduc_sum = nn.BCEWithLogitsLoss(reduction='sum') # sum of losses (instead of mean) over the batch\n","    if args.cuda:\n","        criterion_reduc_sum = criterion_reduc_sum.cuda()\n","    was_training = model.training # don't forget to put it back in training mode at the end!\n","    model.eval()\n","    with torch.no_grad():\n","        predicates_all = []\n","        target_all = []\n","        accumulated_loss = 0\n","        tot_samples = 0\n","        for i_batch, data in enumerate(tqdm(val_dataloader)):\n","            inputs, target = data\n","            target = target.float() # for some reason BCEWithLogitsLoss requires target to be float\n","            if args.cuda:\n","            #     inputs, target = inputs.cuda(), target.cuda()\n","                inputs[0] = inputs[0].cuda()\n","                inputs[1] = inputs[1].cuda()\n","                target = target.cuda()\n","            tot_samples += len(target)\n","            logit = model(inputs)\n","            logit = logit.squeeze(1)\n","            assert logit.shape == (target.shape[0],) # val_dataloader.batch_size, except for the last batch\n","\n","            accumulated_loss += criterion_reduc_sum(logit, target).item() # sum of losses (instead of mean) over the batch\n","            predicates = torch.round(torch.sigmoid(logit))\n","\n","            predicates_all.append(predicates)\n","            target_all.append(target)\n","\n","            if args.cuda:\n","                torch.cuda.synchronize()\n","    if was_training:\n","        model.train()\n","\n","    avg_loss = accumulated_loss / tot_samples\n","    predicates_all = torch.cat(predicates_all).cpu()\n","    target_all = torch.cat(target_all).cpu()\n","    accuracy = metrics.accuracy_score(target_all, predicates_all)\n","    f1_score = metrics.f1_score(target_all, predicates_all)\n","    print(f\"Validation - \\\n","        \\n\\t loss: {accumulated_loss / tot_samples}  \\\n","        \\n\\t acc: {accuracy} \\\n","        \\n\\t f1-score: {f1_score} \\\n","    \")\n","    # if args.log_result:\n","    #     with open(os.path.join(path, args.save_folder,'result_res.csv'), 'a') as r:\n","    #         r.write('\\n{:d},{:d},{:.5f},{:.2f},{:f}'.format(epoch_train, \n","    #                                                         batch_train, \n","    #                                                         avg_loss, \n","    #                                                         accuracy, \n","    #                                                         optimizer.state_dict()['param_groups'][0]['lr']))\n","    return avg_loss, accuracy\n","\n","def predict(test_dataloader, model):\n","    assert not test_dataloader.dataset.is_labeled # the samples we get from test_dataloader are inputs only, no labels!\n","    was_training = model.training # don't forget to put it back in training mode at the end!\n","    model.eval()\n","    with torch.no_grad():\n","        y_pred = []\n","        for i_batch, data in enumerate(tqdm(test_dataloader)):\n","            inputs = data\n","            # inputs = inputs[::-1] # TODO: check that it's in the right order\n","            if args.cuda:\n","            #     inputs = inputs.cuda()\n","                inputs[0] = inputs[0].cuda()\n","                inputs[1] = inputs[1].cuda()\n","            logit = model(inputs)\n","            logit = logit.squeeze(1)\n","            assert logit.shape == (inputs.shape[0],) # test_dataloader.batch_size, except for the last batch\n","\n","            predicates = torch.round(torch.sigmoid(logit))\n","            y_pred.append(predicates)\n","\n","            if args.cuda:\n","                torch.cuda.synchronize()\n","    if was_training:\n","        model.train()\n","\n","    y_pred = torch.cat(y_pred)\n","    return y_pred"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-SB44MGJGLqU","colab_type":"text"},"source":["### Define (and instantiate) the model"]},{"cell_type":"code","metadata":{"id":"gftPUgiGiGWD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882124534,"user_tz":-120,"elapsed":43099,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["\"\"\"input size (N x C x L), for example, the initial text is (batch_size, 70, 501)\"\"\"\n","\n","def conv3(in_features, out_features, stride=1, padding=1, dilation=1, groups=1):\n","    \"\"\"(1D-)Convolution with kernel size 3, with padding\n","    Args:\n","        in_features: nb input channels,\n","        out_features: nb output channels.\n","    \"\"\"\n","    return nn.Conv1d(in_features, out_features, kernel_size=3, stride=stride,\n","                     padding=padding, dilation=dilation, groups=groups, bias=True)\n","\n","def conv1(in_features, out_features, stride=1):\n","    \"\"\"(1D-)Convolution with kernel size 1\"\"\"\n","    return nn.Conv1d(in_features, out_features, kernel_size=1, stride=stride, bias=True)\n","\n","class  CharResCNN_pre(nn.Module):\n","    def __init__(self, out_dim=60, nb_feats=70):\n","        \"\"\"Almost the same as CharResCNN, but the output dimension is >1 (not yet logits)\"\"\"\n","        super().__init__()\n","        self.out_dim = out_dim\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool1d = nn.MaxPool1d(kernel_size=3, stride=3)\n","        # self.extension1 = conv1(128, 256)\n","        # self.extension2 = conv1(256, 512)\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(nb_feats, 256, kernel_size=7, stride=1),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool1d(kernel_size=3, stride=3)\n","        )\n","        self.downsample = nn.Sequential(\n","            conv1(256, 512, stride=2),\n","            nn.BatchNorm1d(512)\n","        )         \n","        self.res3 = nn.Sequential(\n","            conv3(256, 256),\n","            nn.BatchNorm1d(256),\n","        )\n","        self.res4 = nn.Sequential(\n","            conv3(256, 256),\n","            nn.BatchNorm1d(256),\n","        )\n","        self.res5 = nn.Sequential(\n","            conv3(256, 512, stride=2),\n","            nn.BatchNorm1d(512),\n","        )\n","        self.res6 = nn.Sequential(\n","            conv3(512, 512),\n","            nn.BatchNorm1d(512),\n","        )\n","        # After ResConv, shape = [B, C, L] = [B, 256, 18x3] B:Batch size\n","        \"\"\"\n","        Input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), \n","        B is the batch size, and * is any number of dimensions (including 0). \n","        If batch_first is True, B x T x * input is expected.\n","        \"\"\"\n","        self.rnn = nn.GRU(input_size=512,\n","\t\t\t\t\t\t  hidden_size=50,\n","\t\t\t\t\t\t  num_layers=2,\n","\t\t\t\t\t\t  bidirectional=True,\n","\t\t\t\t\t\t  batch_first=True,\n","\t\t\t\t\t\t  dropout=0.5)\n","        \"\"\"\n","        GRU input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence.\n","        \"\"\"\n","\t\t    # self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","        # self.out1 = nn.Linear(1024, 512)\n","        # self.out2 = nn.Linear(512, 1)\n","        # self.dropout = nn.Dropout(0.5)\n","        self.lin_each_state = nn.Linear(100, 30)\n","        self.flatten = nn.Flatten()\n","        # self.out2 = nn.Linear(810, 1)\n","        self.out2 = nn.Linear(810, out_dim)\n","\n","        # 9216 = 256*36\n","        # 4352 = 256*17\n","        # m = nn.MaxPool1d(3, stride=3)\n","        # input = torch.randn(20, 16, 53)\n","        # output = m(input)     \n","        # output.size()=17!!!!!\n","\n","    def forward(self, x):\n","        \"\"\"\n","        tweet = length of 501\n","        one-hot encoding = [70, 501]\n","        into CNN [B, C, H, W] ---> 1D conv: [B, C, L] ---> [64, 70, 501] ---> conv 3x3, do it on the last dim (length) \n","          ---> if kernel is 3x3, actually is [70, 3, 3], number of kernel is the output channel \n","        \"\"\"\n","        # (batch, seqlen, nbfeats) -> (batch, nbfeats, seqlen) to feed into Conv1d\n","        x = x.permute(0, 2, 1)\n","\n","        x = self.conv1(x)\n","        # x = self.conv2(x)\n","\n","        identity3 = x\n","        x = self.res3(x)\n","        x += identity3\n","        x = self.relu(x)\n","\n","        identity4 = x\n","        x = self.res4(x)\n","        x += identity4\n","        x = self.relu(x)\n","\n","        identity5 = self.downsample(x)\n","        x = self.res5(x)\n","        x += identity5\n","        x = self.relu(x)\n","\n","        identity6 = x\n","        x = self.res6(x)\n","        x += identity6\n","        x = self.relu(x)\n","        \n","        x = self.maxpool1d(x)\n","        # x: (batch, channels, seqlen)\n","        x = x.permute(0, 2, 1)\n","        # x: (batch, seqlen, channels) = (batch, 27, 512)\n","        x, _ = self.rnn(x)\n","            # # hidden = [n layers * n directions, batch size, emb dim]\n","            # hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n","        # x: (batch, seqlen, hiddenstates) = (batch, 27, 100)\n","        x = self.lin_each_state(x)\n","        # x: (batch, seqlen, hiddenstates) = (batch, 27, 30)\n","        x = self.flatten(x)\n","        # output = self.out1(x)\n","        output = self.out2(x)\n","        # output: (batch size, out_dim)\n","\n","        return output"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYpy3RzpkqK0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882124536,"user_tz":-120,"elapsed":42932,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["\"\"\"input size (N x C x L), for example, the initial text is (batch_size, 200, 131)\"\"\"\n","\n","class  WordCNN_pre(nn.Module):\n","    def __init__(self, out_dim=50, input_feats=200, input_len=131):\n","        \"\"\"Almost the same as WordCNN, but the output dimension is >1 (not yet logits)\n","\n","        Args:\n","            input_feats (int): the expected number of features of the input sequences\n","            input_len (int): the expected length of the input sequences (already padded)\n","        \"\"\"\n","        super().__init__()\n","        self.out_dim = out_dim\n","\n","        self.rnn = nn.GRU(input_size=input_feats,\n","\t\t\t\t\t\t  hidden_size=25,\n","\t\t\t\t\t\t  num_layers=2,\n","\t\t\t\t\t\t  bidirectional=True,\n","\t\t\t\t\t\t  batch_first=True,\n","\t\t\t\t\t\t  dropout=0.5)\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(50, 50, 3), # nb of input channels is 2*hidden_size of rnn (2* because bidirectional)\n","            nn.BatchNorm1d(50),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=3, stride=3)\n","        )\n","        self.downsample = nn.Sequential(\n","            nn.Conv1d(50, 50, 4, stride=4),\n","            nn.BatchNorm1d(50)\n","        )\n","        self.fc_final = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(500, 50),\n","            nn.ReLU(),\n","            # nn.Linear(50, 1)\n","            nn.Linear(50, out_dim)\n","        )\n","\n","    def forward(self, x):\n","        # x: (batch, input_len, input_feats) = (batch, 131, 200)\n","        out, _ = self.rnn(x) # don't need the final hidden state\n","        # out: (batch, seq_len, num_directions * hidden_size)\n","        out = out.permute(0, 2, 1)\n","        # out: (batch, num_directions * hidden_size, seq_len) = (batch, 60, 131)\n","        out = self.conv1(out)\n","        # out: (batch, out_channels, out_len) = (batch, 70, 43)\n","        out = self.downsample(out)\n","        # out: (batch, out_channels, out_len) = (batch, 70, 10)\n","        out = self.fc_final(out)\n","        # out: (batch, out_dim)\n","        return out"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"cX-fwGXci9xq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882124539,"user_tz":-120,"elapsed":42774,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["\"\"\"input size (N x C x L), for example, the initial text is (batch_size, 200, 131)\"\"\"\n","\n","class  CharAndWordCNN(nn.Module):\n","    def __init__(self, charCNN_out_dim=60, wordCNN_out_dim=50, raw_nb_feats=70, pro_input_feats=200, pro_input_len=131):\n","        \"\"\"Almost the same as WordCNN, but the output dimension is >1 (not yet logits)\n","\n","        Args:\n","            input_feats (int): the expected number of features of the input sequences\n","            input_len (int): the expected length of the input sequences (already padded)\n","        \"\"\"\n","        super().__init__()\n","        self.charCNN = CharResCNN_pre(out_dim=charCNN_out_dim, nb_feats=raw_nb_feats)\n","        self.wordCNN = WordCNN_pre(out_dim=wordCNN_out_dim, input_feats=pro_input_feats, input_len=pro_input_len)\n","        self.fc_combine = nn.Sequential(\n","            nn.Linear(charCNN_out_dim + wordCNN_out_dim, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, 1)\n","        )\n","\n","    def forward(self, x):\n","        x_raw, x_pro = x\n","        out_charCNN = self.charCNN(x_raw) # (batch, charCNN_out_dim)\n","        out_wordCNN = self.wordCNN(x_pro) # (batch, wordCNN_out_dim)\n","        out = torch.cat((out_charCNN, out_wordCNN), dim=1) \n","        out = self.fc_combine(out)\n","        # out: (batch, 1)\n","        return out"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFxT4MbysQPn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595882134115,"user_tz":-120,"elapsed":52219,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"6ef6f25c-f235-4608-837b-7dcbb0e1ee33"},"source":["model = CharAndWordCNN()\n","model_nickname = \"CharAndWordCNN\"\n","print(f\"{count_parameters(model)} parameters\")\n","\n","criterion = nn.BCEWithLogitsLoss()\n","if args.cuda:\n","    model = torch.nn.DataParallel(model).cuda()\n","    # model = model.cuda()\n","    criterion = criterion.cuda()\n","\n","optimizer = optim.Adam(model.parameters()) # TODO: try tweaking parameters (e.g learning rate)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["2215079 parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6R_gzmAlGXyK","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"ruat10cbK-hQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882197102,"user_tz":-120,"elapsed":488,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["args.checkpoint_continue_from = os.path.join(ROOT_PATH, \"CIL-results\", \"my_checkpoints\", \"CharAndWordCNN_epoch_10_2020-07-27T19:51:08.351260.pth.tar\")\n","# args.checkpoint_continue_from = None"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMsn-Lvm_hcS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1595882198591,"user_tz":-120,"elapsed":1536,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"1c593be3-e0f5-4c7c-dac2-8e030fa24ab0"},"source":["if args.checkpoint_continue_from:\n","    print(f\"=> loading checkpoint from {args.checkpoint_continue_from}\")\n","    checkpoint = load_checkpoint(model, optimizer, args.checkpoint_continue_from) # load the state to `model` and `optimizer` and fetch the remaining info into `checkpoint`\n","    \n","    # always assume that we saved a model after an epoch finished, so start at the next epoch.\n","    start_epoch = checkpoint['epoch'] + 1\n","    # load optimizer, default all parameters are in cpu     --> pretty sure it's always a noop, but just in case\n","    if args.cuda:\n","        for state in optimizer.state.values():\n","            for k, v in state.items():\n","                if torch.is_tensor(v):\n","                    state[k] = v.cuda()\n","else:\n","    start_epoch = 1\n","\n","start_epoch"],"execution_count":22,"outputs":[{"output_type":"stream","text":["=> loading checkpoint from /content/drive/My Drive/cil/CIL-results/my_checkpoints/CharAndWordCNN_epoch_10_2020-07-27T19:51:08.351260.pth.tar\n","successfully loaded model and optimizer states from checkpoint (in place)\n","the model was trained for 10 epochs\n","the model had achieved validation accuracy 0.8442\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"CFTI0jfZKDWE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882134120,"user_tz":-120,"elapsed":51591,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["# ## for a manual save\n","# epoch=\"SPECIAL\"\n","\n","# if args.checkpoint_save_to_dir:\n","#     ts = datetime.datetime.now().isoformat()\n","#     file_path = os.path.join(args.checkpoint_save_to_dir, f\"{model_nickname}_epoch_{epoch}_{ts}.pth.tar\")\n","#     print(f\"=> saving checkpoint model to {file_path}\")\n","#     save_checkpoint(model, \n","#                     optimizer,\n","#                     {'epoch': epoch,\n","#                       'validation_accuracy': val_acc},\n","#                     file_path)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3EIH6Z3gWWT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882134120,"user_tz":-120,"elapsed":51426,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["# ## for a manual validation evaluation\n","# val_loss, val_acc = eval(val_dataloader, model)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34fyLo5crkOT","colab_type":"text"},"source":["#### Train the whole model at once"]},{"cell_type":"code","metadata":{"id":"LIISjeh9rfts","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":480,"referenced_widgets":["f2b601ed7af54d06abdf11101bc44093","454a143d38854c0386470c27113fba98","cba10a188d0e4cbcbbbfa3ae2dff3c00","27c4ee22a1074e8d8462740c51424443","329ce3c3b64d4e51b6ebc30acc571768","ab9fdf0e3ce0409ba07d4ba3292343c6","fd69aecc2ef446a889c9c53024523dac","b6ed826e11314c7d9fe4e521abacf6c9"]},"executionInfo":{"status":"error","timestamp":1595882149353,"user_tz":-120,"elapsed":66337,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"c0248d83-ee16-490f-930f-c6528554e822"},"source":["model.train()\n","\n","for epoch in range(start_epoch, args.epochs+1):\n","    print(f\"\\n\\n===== Starting epoch #{epoch} =====\")\n","    accumulated_train_loss = 0\n","    for i_batch, data in enumerate(tqdm(train_dataloader)):\n","        inputs, target = data\n","        target = target.float() # for some reason BCEWithLogitsLoss requires target to be float\n","        if args.cuda:\n","            inputs[0] = inputs[0].cuda()\n","            inputs[1] = inputs[1].cuda()\n","            target = target.cuda()\n","\n","        optimizer.zero_grad()\n","        logit = model(inputs)\n","        logit = logit.squeeze(1) # (n, 1) -> (n,)\n","        assert logit.shape == (target.shape[0],) # train_dataloader.batch_size, except for the last batch\n","        loss = criterion(logit, target)\n","        accumulated_train_loss += criterion(logit, target).item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n","        optimizer.step()\n","        if args.cuda:\n","            torch.cuda.synchronize()\n","\n","        # if args.verbose:\n","        #     print('\\nTargets, Predicates')\n","        #     print(torch.cat((target.unsqueeze(1), torch.unsqueeze(torch.max(logit, 1)[1].view(target.size()).data, 1)), 1))\n","        #     print('\\nLogit')\n","        #     print(logit)\n","        # if i_batch % args.log_interval == 0:\n","        #     corrects = (torch.round(torch.sigmoid(logit)) == target.data).float().sum()  # convert into float for division\n","        #     accuracy = 100.0 * corrects/args.batch_size\n","        #     print('Epoch[{}] Batch[{}] - loss: {:.6f}  lr: {:.5f}  acc: {:.3f}% ({}/{})'.format(epoch,\n","        #                                                                   i_batch,\n","        #                                                                   loss.data,\n","        #                                                                   optimizer.state_dict()['param_groups'][0]['lr'],\n","        #                                                                   accuracy,\n","        #                                                                   corrects,\n","        #                                                                   args.batch_size))\n","        if (i_batch+1) % args.val_interval == 0:\n","            print(f\"Training - loss: {accumulated_train_loss / (i_batch+1)}\")\n","            val_loss, val_acc = eval(val_dataloader, model)\n","\n","    print(f\"----- Finished epoch #{epoch} -----\")\n","    # validation\n","    print('\\nTraining - loss: {:.6f}'.format(accumulated_train_loss/i_batch))\n","    val_loss, val_acc = eval(val_dataloader, model)\n","\n","    # save the model as this epoch\n","    if args.checkpoint_save_to_dir:\n","        ts = datetime.datetime.now().isoformat()\n","        file_path = os.path.join(args.checkpoint_save_to_dir, f\"{model_nickname}_epoch_{epoch}_{ts}.pth.tar\")\n","        print(f\"=> saving checkpoint model to {file_path}\")\n","        save_checkpoint(model, \n","                        optimizer,\n","                        {'epoch': epoch,\n","                         'validation_accuracy': val_acc},\n","                        file_path)\n","\n","    start_epoch = epoch+1\n","\n","print(f\"finished the required number of epochs args.epoch={args.epoch}\")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["\n","\n","===== Starting epoch #1 =====\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2b601ed7af54d06abdf11101bc44093","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=17579.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-a3aa7048bf5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n, 1) -> (n,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train_dataloader.batch_size, except for the last batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-7407cd60dc0d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout_charCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, charCNN_out_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout_wordCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pro\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, wordCNN_out_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_charCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_wordCNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-a074d4dfcb24>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# x: (batch, input_len, input_feats) = (batch, 131, 200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# don't need the final hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# out: (batch, seq_len, num_directions * hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 727\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"SS8o2oaYroWo","colab_type":"text"},"source":["#### Train/Evaluate only the charCNN part"]},{"cell_type":"code","metadata":{"id":"LGz1pOjoru_K","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1595882149349,"user_tz":-120,"elapsed":66028,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["# freeze the parameters that are not in the charCNN part\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVVfBlDMrr2P","colab_type":"text"},"source":["#### Train/Evaluate only the wordCNN part"]},{"cell_type":"code","metadata":{"id":"k1Z28EWav-MU","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1595882149352,"user_tz":-120,"elapsed":65172,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["# symetric of the \"train only charCNN\" section"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAjleEHiv-Vj","colab_type":"text"},"source":["### Evaluate reliance on one method or the other"]},{"cell_type":"markdown","metadata":{"id":"7Jvk8-UYwHpr","colab_type":"text"},"source":["Idea: evaluate on validation samples, but put in garbage instead of the raw inputs, and see how well the model does. If it doesn't do worse than normal, then it relies much more on the `WordCNN_pre` part than on the `CharResCNN_pre` part. Symetrically when putting in garbage instead of the processed inputs."]},{"cell_type":"code","metadata":{"id":"Ci6pU1MnwGfm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595882220105,"user_tz":-120,"elapsed":625,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}}},"source":["def get_garbage_like(inp):\n","    \"\"\"Given a true input vector, returns some garbage that can replace input. Namely, return a random Gaussian vector of the same shape.\n","    The magnitude is chosen s.t expected squared-Frobenius-norm of the garbage equals squared-Frobenius-norm of the true input.\n","    Args:\n","        inp (torch.Tensor): true input to be replaced by garbage\n","    \"\"\"\n","    magnitude = torch.norm(inp) / np.sqrt(inp.numel()) \n","    garb = torch.randn_like(inp) * magnitude\n","    # print(f\"norm of true input: {torch.norm(inp).item()},  norm of garbage: {torch.norm(garb).item()}\") # --> yep, norms are consistently close\n","    return garb\n","\n","def eval_half_garbage(val_dataloader, model, garbagify):\n","    \"\"\"Evaluate on validation but put in garbage instead of sample for one of the methods\n","    Args:\n","        val_dataloader\n","        model\n","        garbagify (str): decide which one of the methods to invalidate\n","            - \"char[CNN]\" or \"raw\":              put in garbage at the input of the charCNN part\n","            - \"word[CNN]\" or \"[pre]pro[cessed]\": put in garbage at the input of the wordCNN part\n","    \"\"\"\n","    if garbagify in [\"char\", \"charCNN\", \"raw\"]:\n","        garbagify = \"raw\"\n","    elif garbagify in [\"word\", \"wordCNN\", \"pro\", \"prepro\", \"preprocessed\"]:\n","        garbagify = \"pro\"\n","    else:\n","        raise ValueError('Argument `garbagify` must be one of \"char[CNN]\"/\"raw\" or \"word[CNN]\"/\"[pre]pro[cessed]\"')\n","    criterion_reduc_sum = nn.BCEWithLogitsLoss(reduction='sum') # sum of losses (instead of mean) over the batch\n","    if args.cuda:\n","        criterion_reduc_sum = criterion_reduc_sum.cuda()\n","    was_training = model.training # don't forget to put it back in training mode at the end!\n","    model.eval()\n","    with torch.no_grad():\n","        predicates_all = []\n","        target_all = []\n","        accumulated_loss = 0\n","        tot_samples = 0\n","        for i_batch, data in enumerate(tqdm(val_dataloader)):\n","            inputs, target = data\n","            target = target.float() # for some reason BCEWithLogitsLoss requires target to be float\n","            if garbagify == \"raw\":\n","                inputs[0] = get_garbage_like(inputs[0])\n","            else:\n","                inputs[1] = torch.randn_like(inputs[1])\n","            if args.cuda:\n","            #     inputs, target = inputs.cuda(), target.cuda()\n","                inputs[0] = inputs[0].cuda()\n","                inputs[1] = inputs[1].cuda()\n","                target = target.cuda()\n","            tot_samples += len(target)\n","            logit = model(inputs)\n","            logit = logit.squeeze(1)\n","            assert logit.shape == (target.shape[0],) # val_dataloader.batch_size, except for the last batch\n","\n","            accumulated_loss += criterion_reduc_sum(logit, target).item() # sum of losses (instead of mean) over the batch\n","            predicates = torch.round(torch.sigmoid(logit))\n","\n","            predicates_all.append(predicates)\n","            target_all.append(target)\n","\n","            if args.cuda:\n","                torch.cuda.synchronize()\n","    if was_training:\n","        model.train()\n","\n","    avg_loss = accumulated_loss / tot_samples\n","    predicates_all = torch.cat(predicates_all).cpu()\n","    target_all = torch.cat(target_all).cpu()\n","    accuracy = metrics.accuracy_score(target_all, predicates_all)\n","    f1_score = metrics.f1_score(target_all, predicates_all)\n","    print(f\"Validation - \\\n","        \\n\\t loss: {accumulated_loss / tot_samples}  \\\n","        \\n\\t acc: {accuracy} \\\n","        \\n\\t f1-score: {f1_score} \\\n","    \")\n","    # if args.log_result:\n","    #     with open(os.path.join(path, args.save_folder,'result_res.csv'), 'a') as r:\n","    #         r.write('\\n{:d},{:d},{:.5f},{:.2f},{:f}'.format(epoch_train, \n","    #                                                         batch_train, \n","    #                                                         avg_loss, \n","    #                                                         accuracy, \n","    #                                                         optimizer.state_dict()['param_groups'][0]['lr']))\n","    return avg_loss, accuracy"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6kXeI4F-wkC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139,"referenced_widgets":["17143582913e4107a590a6533c72f63f","bf5bfcb7de1c44248152d3a7276d44dc","00d9d6f06b524088ad04f14a600ef2a1","1d21f2399d864e72804e60bbc9a75897","1d86802c78b44715989d810c91088af1","5e60e37ac9e0459ba4600e073ffc07f2","b90d607307514e3bbcb7ddf804a777d7","49b262cc5c6c499a977ad82124ebdb19"]},"executionInfo":{"status":"ok","timestamp":1595882854473,"user_tz":-120,"elapsed":30458,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"0ddb5308-3074-4417-d323-c605b98b4b28"},"source":["## manual validation evaluation\n","val_loss, val_acc = eval_half_garbage(val_dataloader, model, garbagify='char')"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17143582913e4107a590a6533c72f63f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=196.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Validation -         \n","\t loss: 0.3503064520263672          \n","\t acc: 0.84856         \n","\t f1-score: 0.8517038777908342     \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j-Q07HVCCBcb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139,"referenced_widgets":["4db8b704d94e4b6082c063dc2125601d","75e221184d6e4c939a94974dd6a97c56","961a2e1aacb5475e96fd2287c1fbafda","957e2fef4e0e47b4afdb6b25e88c8fab","d45ef604ec3f446bb12b9758278d8fb3","2e787eb186c04e638044e9b20d399031","85fdcf09b39f45f5b8b84b1c995c592e","90e8857c563c451486918be92bdc98da"]},"executionInfo":{"status":"ok","timestamp":1595882897978,"user_tz":-120,"elapsed":25962,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"5386f898-fcce-4d57-aa12-c2398f014b1d"},"source":["val_loss, val_acc = eval_half_garbage(val_dataloader, model, garbagify='word')"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4db8b704d94e4b6082c063dc2125601d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=196.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Validation -         \n","\t loss: 11.320286474609375          \n","\t acc: 0.50024         \n","\t f1-score: 0.0     \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jkciUVblF9MA","colab_type":"text"},"source":["### Prediction"]},{"cell_type":"code","metadata":{"id":"5y1PrwMTBbYL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["59708c25cd4a4c02ae93e7d5230c17f1","afd9456eff64472983d6baca34ab6b9f","28482df924a2402caffe14fbb3c01ba6","69425015db064e2dac605622cd552702","c00446e369e2456e99526ac519c9a2e1","fa89ea5bb80b4e1188e89e1ca47a92a7","0e96860aa7ec48eaaf8ee4b694e0d594","748daa432f954f34acd2af81f64f72ad"]},"executionInfo":{"status":"ok","timestamp":1595857534215,"user_tz":-120,"elapsed":7824,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"3336f993-b578-4080-8c92-0ee78071a75d"},"source":["y_pred = predict(test_dataloader, model)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59708c25cd4a4c02ae93e7d5230c17f1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6P8Sthx_BMY0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595857534218,"user_tz":-120,"elapsed":6763,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"8f70b0e0-5d4d-4d7a-c004-5047ee37bc84"},"source":["y_pred = y_pred.cpu().numpy()\n","y_pred = y_pred.astype(np.integer, copy=False)\n","\n","# y_pred[y_pred==1] = -1\n","# y_pred[y_pred==0] = 1\n","y_pred[y_pred==0] = -1\n","print(f\"predict {np.count_nonzero(y_pred==-1)} positive, {np.count_nonzero(y_pred==1)} negative\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict 4808 positive, 5192 negative\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qd3oKZGgBWcT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595857534219,"user_tz":-120,"elapsed":5774,"user":{"displayName":"Guillaume Wang","photoUrl":"","userId":"15258305767846400068"}},"outputId":"cb225c52-fc5e-4977-fa3a-b3d3292caa04"},"source":["ts = datetime.datetime.now().isoformat()\n","SUBMISSION_FILENAME = os.path.join(ROOT_PATH, f\"{model_nickname}_submission_{ts}.csv\")\n","\n","with open(SUBMISSION_FILENAME, \"w\") as f:\n","    f.write(\"Id,Prediction\\n\")\n","    for i, label in enumerate(y_pred, start=1):  \n","        f.write(f\"{i},{label}\\n\")\n","\n","print(f\"wrote to {SUBMISSION_FILENAME}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["wrote to /content/drive/My Drive/cil/WordCNN_submission_2020-07-27T13:45:33.598866.csv\n"],"name":"stdout"}]}]}